{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic algorithm to optimize investment in the stock market"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from deap import base\n",
    "from deap import creator\n",
    "from deap import tools\n",
    "from deap import algorithms\n",
    "import itertools\n",
    "from deap import benchmarks\n",
    "from deap.benchmarks.tools import diversity, convergence, hypervolume\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign problem hyperparameters\n",
    "\n",
    "Two types of hyperparameters exist:\n",
    " - **Fixed parameters:** defined in the project statement\n",
    " - **Test parameters:** defined by us to test different implementations of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED PARAMETERS\n",
    "POPULATION_SIZE = 20 # number of individuals in population\n",
    "GENERATIONS = 10000   # number of generations\n",
    "EVALUATIONS = 10000   # number of evaluations\n",
    "MAX_RUNS = 30 # number of runs with different random seeds\n",
    "\n",
    "# TEST PARAMETERS\n",
    "CROSSOVER_PROBABILITY = 0.9 # probability of crossover operation\n",
    "MUTATION_PROBABILITY = 0.3 # probability of mutation operation\n",
    "TOURNAMENT_SIZE = 5 # number of individuals participating in tournament selection\n",
    "NGSA2_POPULATION_SIZE = 100 # number of individuals in front for NSGA-II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process datasets\n",
    "\n",
    "Filter the data relevant for the problem in question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    # only relevant from 01/01/2020 onwards to 31/12/2022 (3 years)\n",
    "    # convert to datetime format\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%d/%m/%Y')\n",
    "    data = data[data['Date'] >= '01-01-2020']\n",
    "    data = data[data['Date'] <= '31-12-2022']\n",
    "\n",
    "    # calculate the difference between consecutive values in the 'Close' column\n",
    "    value_diff = data['Close'].diff()\n",
    "    data['Value_Diff'] = value_diff\n",
    "\n",
    "    # create 'Gain' and 'Loss' columns based on the 'Value_Diff'\n",
    "    gain = value_diff.apply(lambda x: max(0, x))\n",
    "    loss = value_diff.apply(lambda x: max(0, -x))\n",
    "\n",
    "    data['Loss_sum'] = 0\n",
    "    for index, row in data.iterrows():\n",
    "        row = row['Value_Diff']\n",
    "        if row<0:\n",
    "            loss_sum += row\n",
    "            data['Loss_sum'][index] = loss_sum\n",
    "        else:\n",
    "            loss_sum = 0\n",
    "            data['Loss_sum'][index] = loss_sum\n",
    "    \n",
    "\n",
    "\n",
    "    # calculate the rolling sum of 'Gain' and 'Loss' for a 7-day, 14-day, and 21-day periods\n",
    "    average_gain_7 =  gain.rolling(window=7).mean()\n",
    "    average_loss_7 =  loss.rolling(window=7).mean()\n",
    "    average_gain_14 = gain.rolling(window=14).mean()\n",
    "    average_loss_14 = loss.rolling(window=14).mean()\n",
    "    average_gain_21 = gain.rolling(window=21).mean()\n",
    "    average_loss_21 = loss.rolling(window=21).mean()\n",
    "\n",
    "\n",
    "    # calculate the Relative Strength (RS)\n",
    "    rs_7 = average_gain_7/average_loss_7\n",
    "    rs_14 = average_gain_14/average_loss_14\n",
    "    rs_21 = average_gain_21/average_loss_21\n",
    "\n",
    "    # calculate the Relative Strength Index (RSI)\n",
    "    data['RSI7'] = 100 - (100/(1+rs_7))\n",
    "    data['RSI14'] = 100 - (100/(1+rs_14))\n",
    "    data['RSI21'] = 100 - (100/(1+rs_21))\n",
    "\n",
    "    # Reset the index of the DataFrame\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize DEAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the fitness function\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,1.0))\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# define the parameters\n",
    "rsi_periods = [7, 14, 21]\n",
    "lower_range = list(range(0, 101, 5))\n",
    "upper_range = list(range(0, 101, 5))\n",
    "\n",
    "# create the toolbox\n",
    "toolbox = base.Toolbox()\n",
    "\n",
    "toolbox.register(\"rsi_period\", random.choice, rsi_periods)\n",
    "toolbox.register(\"bound\", random.choice, lower_range)\n",
    "\n",
    "toolbox.register(\"attr_pool\", tools.initCycle, tuple, [toolbox.rsi_period,\n",
    "                                                       toolbox.bound,\n",
    "                                                       toolbox.bound])\n",
    "\n",
    "tools.initCycle\n",
    "tools.initRepeat\n",
    "\n",
    "toolbox.register(\"individual\", tools.initRepeat, creator.Individual, toolbox.attr_pool, n=2)\n",
    "toolbox.register(\"population\",tools.initRepeat, list, toolbox.individual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define simulation function\n",
    "\n",
    "The simulation will be used in the **epigenesis** stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def simulate_behaviour(individual, data):\n",
    "    # extract the parameters from the individual chromosome\n",
    "    (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long) = individual\n",
    "    \n",
    "    rsi_period_long = \"RSI\" + str(rsi_period_long)\n",
    "    rsi_period_short = \"RSI\" + str(rsi_period_short)\n",
    "\n",
    "    # based on the parameters, choose dates to buy and sell\n",
    "    long_buys = data[(data[rsi_period_long] >= lower_long) & \n",
    "                              ((data[rsi_period_long].shift(1) < lower_long) |\n",
    "                               (data[rsi_period_long].shift(1) == np.nan))].copy()\n",
    "    \n",
    "    short_sells = data[(data[rsi_period_short] <= upper_short) &\n",
    "                                ((data[rsi_period_short].shift(1) > upper_short) |\n",
    "                                 (data[rsi_period_short].shift(1) == np.nan))].copy()\n",
    "    \n",
    "    long_sells = []\n",
    "    # check for each long buy the next long sell\n",
    "    for index, row in long_buys.iterrows():\n",
    "        candidate_sells = data[(data['Date'] > row['Date']) &\n",
    "                               (data[rsi_period_long] >= upper_long)]\n",
    "        if (candidate_sells.empty):\n",
    "            new_sell = dict(data.iloc[-1])\n",
    "            new_sell['index'] = len(data) - 1\n",
    "        else:\n",
    "            new_sell = dict(candidate_sells.iloc[0])\n",
    "            new_sell['index'] = candidate_sells.index[0]\n",
    "        long_sells.append(new_sell)\n",
    "        drop_rows = [i for i in long_buys.index if i > index and i < new_sell['index']]\n",
    "        long_buys.drop(drop_rows, inplace=True) \n",
    "\n",
    "    # check for each short sell the next short buy\n",
    "    short_buys = []\n",
    "    for index, row in short_sells.iterrows():\n",
    "        candidate_buys = data[(data['Date'] > row['Date']) &\n",
    "                              (data[rsi_period_short] <= lower_short)]\n",
    "        if (candidate_buys.empty):\n",
    "            new_buy = dict(data.iloc[-1])\n",
    "            new_buy['index'] = len(data) - 1\n",
    "        else:\n",
    "            new_buy = dict(candidate_buys.iloc[0])\n",
    "            new_buy['index'] = candidate_buys.index[0]\n",
    "\n",
    "        short_buys.append(new_buy)\n",
    "        drop_rows = [i for i in short_sells.index if i > index and i < new_buy['index']]\n",
    "        short_sells.drop(drop_rows, inplace=True)\n",
    "\n",
    "\n",
    "    long_sells = pd.DataFrame(long_sells)\n",
    "    short_buys = pd.DataFrame(short_buys)\n",
    "\n",
    "    # remove duplcates \n",
    "    if not long_sells.empty:\n",
    "        long_sells.drop_duplicates(subset='index', inplace=True)\n",
    "        long_sells.set_index('index', inplace=True)\n",
    "    else:\n",
    "        long_sells = pd.DataFrame(columns=data.columns)\n",
    "    # remove duplicates\n",
    "    if not short_buys.empty:\n",
    "        short_buys.drop_duplicates(subset='index', inplace=True)\n",
    "        short_buys.set_index('index', inplace=True)\n",
    "    else:\n",
    "        short_buys = pd.DataFrame(columns=data.columns)\n",
    "\n",
    "    # merge buys and sells (buy long, sell long), (sell short, buy short)\n",
    "    trades_long = pd.merge_asof(long_buys, long_sells, \n",
    "                                left_index=True, right_index=True, \n",
    "                                direction='forward', suffixes=('_buy', '_sell'))\n",
    "    \n",
    "    trades_short = pd.merge_asof(short_sells, short_buys,\n",
    "                                 left_index=True, right_index=True,\n",
    "                                 direction='forward', suffixes=('_sell', '_buy'))\n",
    "\n",
    "    return (trades_long, trades_short)\n",
    "\n",
    "\n",
    "# trades_long, trades_short = simulate_behaviour(individual, data)\n",
    "\n",
    "# # plot evolution of RSI and points where buys and sells are possible\n",
    "# plt.figure(figsize=(20,10))\n",
    "# plt.plot(data['Date'], data['Close'], label='Close')\n",
    "# plt.plot(trades_long['Date_buy'], trades_long['Close_buy'], 'ro', label='Buys Long')\n",
    "# plt.plot(trades_long['Date_sell'], trades_long['Close_sell'], 'go', label='Sells Long')\n",
    "# plt.plot(trades_short['Date_sell'], trades_short['Close_sell'], 'bo', label='Sells Short')\n",
    "# plt.plot(trades_short['Date_buy'], trades_short['Close_buy'], 'yo', label='Buys Short')\n",
    "# plt.legend(loc='upper left')\n",
    "# plt.show()\n",
    "\n",
    "toolbox.register(\"simulate\", simulate_behaviour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define objective function\n",
    "\n",
    "The objective function will be used in the **selection** stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_results(trades_long, trades_short, data):\n",
    "    # calculate the return on investment (ROI) for each trade\n",
    "    trades_long['ROI'] = (trades_long['Close_sell'] - trades_long['Close_buy']) / trades_long['Close_buy'] * 100\n",
    "    trades_short['ROI'] = (trades_short['Close_sell'] - trades_short['Close_buy']) / trades_short['Close_sell'] * 100\n",
    "\n",
    "    drawdowns = []\n",
    "    # for each transaction, calculate the drawdown\n",
    "    trades_long['Date_buy'] = pd.to_datetime(trades_long['Date_buy'], format='%d/%m/%Y')\n",
    "    trades_long['Date_sell'] = pd.to_datetime(trades_long['Date_sell'], format='%d/%m/%Y')\n",
    "    trades_short['Date_buy'] = pd.to_datetime(trades_short['Date_buy'], format='%d/%m/%Y')\n",
    "    trades_short['Date_sell'] = pd.to_datetime(trades_short['Date_sell'], format='%d/%m/%Y')\n",
    "    \n",
    "    # for long, short in zip(trades_long, trades_short):\n",
    "    #     value = min( data ['Loss_sum'] [ data['Date'] > long['Date_buy'] & data['Date'] < long['Date_sell'] ])\n",
    "    #     value2 = min( data ['Loss_sum'] [ data['Date'] > short['Date_buy'] & data['Date'] < short['Date_sell'] ])\n",
    "    #     drawdown = min(value, value2) # minimum drawdown of the two trades\n",
    "    #     drawdowns.append(drawdown)\n",
    "\n",
    "    # minimum = min(drawdowns) # minimum drawdown of all trades\n",
    "    for long, short in zip(trades_long.itertuples(), trades_short.itertuples()):\n",
    "        # Filter the data based on date ranges\n",
    "        value = min(data['Loss_sum'] [(data['Date'] >= long.Date_buy) & (data['Date'] <= long.Date_sell)])\n",
    "        value2 = min(data['Loss_sum'] [(data['Date'] >= short.Date_sell) & (data['Date'] <= short.Date_buy)])\n",
    "\n",
    "        # Calculate the minimum drawdown of the two trades\n",
    "        drawdown = min(value, value2)\n",
    "\n",
    "        drawdowns.append(drawdown)\n",
    "\n",
    "    if len(drawdowns) == 0:\n",
    "        minimum = 0\n",
    "    else:\n",
    "        minimum = min(drawdowns) # minimum drawdown of all trades\n",
    "\n",
    "\n",
    "    # calculate the total ROI for each trade type\n",
    "    average_roi_long = trades_long['ROI'].sum()\n",
    "    average_roi_short = trades_short['ROI'].sum()\n",
    "    # calculate the average ROI \n",
    "    average_roi = (average_roi_long + average_roi_short) / 2\n",
    "    \n",
    "\n",
    "    return (average_roi, minimum)\n",
    "\n",
    "toolbox.register(\"evaluate\", evaluate_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define selection method\n",
    "\n",
    "The selection function will be used in the **survival** stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toolbox.register(\"select\", tools.selTournament, tournsize=TOURNAMENT_SIZE)\n",
    "toolbox.register(\"select\", tools.selNSGA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define crossover method\n",
    "\n",
    "The crossover function will be used in the **crossover** stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crossover Hyperparameters\n",
    "UNIFORM_PROBABILITY = 0.5\n",
    "AVERAGE_PROBABILITY = 0.5\n",
    "\n",
    "def crossover(individual1, individual2, method=\"one-point\"):\n",
    "    if method == \"one-point\":\n",
    "        # 1-point crossover\n",
    "        crossover_point = random.randint(0, len(individual1[0]) - 1)\n",
    "        child1 = individual1[:crossover_point] + individual2[crossover_point:]\n",
    "        child2 = individual2[:crossover_point] + individual1[crossover_point:]\n",
    "        return child1, child2\n",
    "    elif method == \"two-point\":\n",
    "        # 2-point crossover\n",
    "        crossover_point1 = random.randint(0, len(individual1[0]) - 1)\n",
    "        crossover_point2 = random.randint(crossover_point1, len(individual1[0]) - 1)\n",
    "        child1 = individual1[:crossover_point1] + individual2[crossover_point1:crossover_point2] + individual1[crossover_point2:]\n",
    "        child2 = individual2[:crossover_point1] + individual1[crossover_point1:crossover_point2] + individual2[crossover_point2:]\n",
    "        return child1, child2\n",
    "    elif method == \"uniform\":\n",
    "        # uniform crossover\n",
    "        child1 = individual1.copy()\n",
    "        child2 = individual2.copy()\n",
    "        for i in range(len(individual1)):\n",
    "            if random.random() < UNIFORM_PROBABILITY:\n",
    "                child1[i], child2[i] = individual2[i], individual1[i]\n",
    "        return child1, child2\n",
    "    elif method == \"average\":\n",
    "        # average crossover\n",
    "        if random.random() < AVERAGE_PROBABILITY:\n",
    "            child1 = (individual1+individual2)/2\n",
    "        return child1, child2\n",
    "    \n",
    "\n",
    "toolbox.register(\"mate\", crossover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define mutation method\n",
    "\n",
    "The mutation function will be used in the **mutation** stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian mutation hyperparameters\n",
    "SIGMA = [0, 20, 20, 0, 20, 20]\n",
    "MU = 0\n",
    "GAUSSIAN_PROBABILITY = 0.7\n",
    "\n",
    "def mutationGaussian(individual):\n",
    "        (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long) = individual\n",
    "        if random.random()<GAUSSIAN_PROBABILITY:\n",
    "            lower_short = np.random.normal(MU, SIGMA[1])\n",
    "            lower_short = max(0, min(100, lower_short))\n",
    "            lower_short = lower_short - (lower_short % 5)\n",
    "        if random.random()<GAUSSIAN_PROBABILITY:\n",
    "            upper_short = np.random.normal(MU, SIGMA[2])\n",
    "            upper_short = max(0, min(100, upper_short))\n",
    "            upper_short = upper_short - (upper_short % 5)\n",
    "        if random.random()<GAUSSIAN_PROBABILITY:\n",
    "            lower_long = np.random.normal(MU, SIGMA[4])\n",
    "            lower_long = max(0, min(100, lower_long))\n",
    "            lower_long = lower_long - (lower_long % 5)\n",
    "        if random.random()<GAUSSIAN_PROBABILITY:\n",
    "            upper_long = np.random.normal(MU, SIGMA[5])\n",
    "            upper_long = max(0, min(100, upper_long))\n",
    "            upper_long = upper_long - (upper_long % 5)\n",
    "\n",
    "        individual = (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long)\n",
    "        return individual,\n",
    "    \n",
    "# Single mutation hyperparameters\n",
    "MUTATION_BOUND = 10 # mutation bound - hyperparameter\n",
    "MUTATION_WINDOW = 7 # windows bound mutation - hyperparameter\n",
    "MUTATION_SINGLE_PROBABILITY = 0.5 # probability of single mutation - hyperparameter\n",
    "\n",
    "def mutationSingle(individual):\n",
    "        # mutation direction is either -1 (decrease) or 1 (increase)\n",
    "        (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long) = individual\n",
    "        mutation_direction = random.choice([-1, 1])\n",
    "        # mutate the parameter, consideration of bounds\n",
    "        if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            rsi_period_short = max(7, min(21, rsi_period_short + mutation_direction * MUTATION_BOUND))\n",
    "        elif random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            lower_short = max(0, min(100, lower_short + mutation_direction * MUTATION_BOUND))\n",
    "        elif random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            upper_short = max(0, min(100, upper_short + mutation_direction * MUTATION_BOUND))\n",
    "        elif random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            rsi_period_long = max(7, min(21, rsi_period_long + mutation_direction *  MUTATION_BOUND))\n",
    "        elif random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            lower_long = max(0, min(100, lower_long + mutation_direction * MUTATION_BOUND))\n",
    "        elif random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "            mutation_direction = random.choice([-1, 1])\n",
    "            upper_long = max(0, min(100, upper_long + mutation_direction * MUTATION_BOUND))\n",
    "        individual = (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long)\n",
    "        return individual,\n",
    "\n",
    "def mutationInversion(individual):\n",
    "    (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long) = individual\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        rsi_period_short = 7 + 21 - rsi_period_short\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        rsi_period_long = 7 + 21 - rsi_period_long\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        lower_short = 100 - lower_short\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        upper_short = 100 - upper_short\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        lower_long = 100 - lower_long\n",
    "    if random.random() < MUTATION_SINGLE_PROBABILITY:\n",
    "        upper_long = 100 - upper_long\n",
    "    individual = (rsi_period_short, lower_short, upper_short), (rsi_period_long, lower_long, upper_long)\n",
    "    return individual,\n",
    "\n",
    "def mutate(individual, method=\"single\"):\n",
    "    if method == \"single\":\n",
    "        mutant = mutationSingle(individual) # single mutation\n",
    "    elif method == \"gaussian\":\n",
    "        mutant = mutationGaussian(individual)\n",
    "    elif method == \"inversion\":\n",
    "        mutant = mutationInversion(individual)\n",
    "        return mutant,\n",
    "\n",
    "toolbox.register(\"mutate\", mutate)\n",
    "\n",
    "# toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "# toolbox.register(\"mutate\", tools.mutUniformInt, low=0, up=100, indpb=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n",
      "/tmp/ipykernel_75003/2152951316.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Loss_sum'][index] = loss_sum\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Run: 0--\n",
      "\n",
      "[[(14, 65, 5), (14, 80, 75)], [(14, 45, 75), (14, 90, 30)], [(21, 20, 45), (7, 15, 95)], [(14, 85, 95), (7, 45, 15)], [(21, 10, 50), (14, 85, 15)], [(14, 65, 50), (21, 100, 30)], [(21, 75, 70), (21, 40, 5)], [(21, 0, 10), (21, 60, 100)], [(7, 95, 75), (14, 35, 50)], [(21, 10, 30), (21, 35, 35)], [(7, 85, 70), (7, 10, 50)], [(21, 75, 15), (14, 85, 45)], [(21, 15, 85), (14, 85, 30)], [(21, 85, 90), (14, 70, 10)], [(21, 60, 50), (21, 35, 45)], [(7, 30, 25), (7, 95, 40)], [(14, 10, 10), (21, 20, 20)], [(7, 10, 85), (21, 60, 80)], [(14, 80, 35), (7, 90, 65)], [(21, 40, 70), (14, 100, 55)], [(7, 50, 95), (7, 75, 90)], [(21, 50, 30), (7, 0, 40)], [(7, 35, 55), (7, 50, 65)], [(7, 15, 20), (21, 35, 5)], [(21, 100, 85), (21, 10, 0)], [(7, 100, 30), (21, 90, 15)], [(14, 10, 55), (7, 5, 95)], [(7, 30, 25), (21, 15, 75)], [(7, 5, 0), (21, 65, 95)], [(7, 40, 10), (7, 10, 100)], [(14, 55, 65), (7, 5, 80)], [(14, 5, 95), (7, 60, 30)], [(14, 55, 75), (21, 25, 30)], [(7, 25, 25), (14, 80, 40)], [(7, 95, 70), (21, 25, 0)], [(14, 65, 90), (21, 45, 100)], [(14, 60, 40), (7, 85, 0)], [(14, 10, 50), (21, 5, 85)], [(14, 20, 35), (14, 55, 95)], [(14, 55, 90), (21, 95, 20)], [(21, 45, 60), (21, 65, 100)], [(7, 0, 95), (7, 50, 25)], [(7, 35, 100), (14, 60, 90)], [(14, 5, 60), (21, 90, 65)], [(21, 5, 25), (14, 10, 40)], [(21, 25, 70), (21, 75, 85)], [(21, 0, 5), (14, 50, 45)], [(14, 5, 65), (7, 85, 100)], [(7, 20, 0), (14, 65, 50)], [(7, 30, 0), (21, 0, 80)], [(21, 15, 30), (7, 95, 100)], [(7, 45, 40), (21, 25, 15)], [(14, 60, 100), (7, 0, 40)], [(14, 15, 40), (7, 100, 80)], [(21, 100, 55), (7, 20, 40)], [(7, 5, 5), (7, 40, 85)], [(14, 55, 90), (7, 95, 100)], [(14, 100, 70), (21, 65, 55)], [(21, 25, 30), (14, 90, 45)], [(7, 20, 20), (14, 50, 50)], [(14, 10, 50), (21, 5, 5)], [(14, 25, 20), (21, 45, 55)], [(14, 85, 20), (14, 15, 75)], [(21, 35, 5), (14, 25, 80)]]\n",
      "Starting evolution...\n",
      "Evaluated 64 individuals\n",
      "--Generation: 0--\n",
      "Evaluated 57 individuals\n",
      "Min: 0.29987707095453653\n",
      "Max: 44.265225798200845\n",
      "Avg: 13.350922238658882\n",
      "Std: 11.9810741182524\n",
      "Current evaluations :  121\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -6.749531218749997\n",
      "Std drawdown: 7.5461311632766055\n",
      "--Generation: 1--\n",
      "Evaluated 64 individuals\n",
      "Min: 6.537505093757946\n",
      "Max: 44.265225798200845\n",
      "Avg: 25.369440906701247\n",
      "Std: 10.33823853004716\n",
      "Current evaluations :  185\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -6.7896872499999965\n",
      "Std drawdown: 7.856442156739336\n",
      "--Generation: 2--\n",
      "Evaluated 60 individuals\n",
      "Min: 31.49913108987596\n",
      "Max: 44.265225798200845\n",
      "Avg: 35.67056979495184\n",
      "Std: 4.530851590946737\n",
      "Current evaluations :  245\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -3.2549999999999972\n",
      "Std drawdown: 6.775822828262256\n",
      "--Generation: 3--\n",
      "Evaluated 58 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 38.996977525220984\n",
      "Std: 4.080767564987105\n",
      "Current evaluations :  303\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -6.509999999999998\n",
      "Std drawdown: 8.40437386127008\n",
      "--Generation: 4--\n",
      "Evaluated 56 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 42.2896326958334\n",
      "Std: 3.5706716193640315\n",
      "Current evaluations :  359\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -13.291250000000003\n",
      "Std drawdown: 7.3538271286113055\n",
      "--Generation: 5--\n",
      "Evaluated 56 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 44.00181338455185\n",
      "Std: 1.4666182494427265\n",
      "Current evaluations :  415\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -16.8175\n",
      "Std drawdown: 3.020512166835206\n",
      "--Generation: 6--\n",
      "Evaluated 59 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 44.00181338455185\n",
      "Std: 1.4666182494427265\n",
      "Current evaluations :  474\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -16.8175\n",
      "Std drawdown: 3.020512166835206\n",
      "--Generation: 7--\n",
      "Evaluated 60 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 44.00181338455185\n",
      "Std: 1.4666182494427265\n",
      "Current evaluations :  534\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -16.8175\n",
      "Std drawdown: 3.020512166835206\n",
      "--Generation: 8--\n",
      "Evaluated 61 individuals\n",
      "Min: 35.83602856143301\n",
      "Max: 44.265225798200845\n",
      "Avg: 44.00181338455185\n",
      "Std: 1.4666182494427265\n",
      "Current evaluations :  595\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -16.8175\n",
      "Std drawdown: 3.020512166835206\n",
      "-- End of (successful) evolution --\n",
      "--Run: 1--\n",
      "\n",
      "[[(7, 90, 10), (14, 15, 75)], [(14, 75, 100), (14, 30, 15)], [(14, 0, 60), (14, 95, 0)], [(21, 70, 40), (21, 35, 90)], [(7, 50, 0), (7, 0, 100)], [(21, 0, 60), (21, 30, 65)], [(21, 0, 80), (7, 70, 75)], [(21, 35, 55), (7, 35, 70)], [(14, 0, 65), (21, 100, 15)], [(7, 100, 45), (7, 50, 80)], [(14, 80, 30), (14, 45, 90)], [(14, 80, 60), (21, 5, 75)], [(7, 60, 65), (21, 25, 55)], [(21, 55, 10), (14, 80, 15)], [(7, 80, 60), (14, 75, 0)], [(14, 5, 45), (21, 95, 90)], [(21, 60, 100), (7, 25, 80)], [(7, 0, 30), (21, 85, 35)], [(14, 80, 55), (21, 55, 70)], [(14, 85, 95), (21, 0, 60)], [(21, 80, 20), (21, 85, 30)], [(14, 5, 75), (14, 90, 85)], [(7, 80, 65), (14, 55, 65)], [(14, 0, 85), (21, 95, 95)], [(14, 70, 95), (7, 35, 100)], [(7, 85, 90), (7, 10, 85)], [(14, 5, 10), (7, 0, 70)], [(7, 40, 35), (14, 15, 95)], [(7, 55, 45), (7, 25, 25)], [(14, 80, 25), (21, 40, 100)], [(21, 45, 70), (21, 50, 75)], [(14, 15, 0), (14, 60, 50)], [(14, 30, 40), (7, 40, 80)], [(7, 95, 65), (7, 35, 0)], [(14, 20, 5), (21, 25, 70)], [(21, 80, 65), (21, 35, 100)], [(21, 80, 70), (7, 80, 100)], [(7, 60, 90), (14, 100, 65)], [(7, 45, 20), (7, 5, 45)], [(7, 10, 45), (14, 25, 65)], [(21, 40, 20), (7, 85, 5)], [(21, 30, 90), (14, 25, 95)], [(21, 5, 60), (7, 55, 15)], [(7, 90, 65), (21, 30, 75)], [(7, 60, 45), (21, 75, 0)], [(14, 95, 60), (14, 0, 25)], [(7, 50, 90), (7, 50, 65)], [(7, 40, 15), (14, 85, 55)], [(21, 85, 75), (21, 35, 10)], [(21, 5, 10), (7, 25, 25)], [(21, 30, 40), (14, 95, 80)], [(14, 55, 50), (14, 15, 45)], [(7, 95, 75), (7, 90, 85)], [(7, 50, 5), (14, 10, 60)], [(7, 20, 50), (7, 95, 90)], [(14, 10, 90), (21, 35, 90)], [(7, 40, 55), (14, 90, 85)], [(7, 70, 40), (7, 5, 45)], [(7, 95, 0), (7, 65, 15)], [(7, 30, 35), (21, 65, 25)], [(7, 70, 25), (21, 35, 25)], [(21, 15, 65), (14, 85, 45)], [(21, 40, 75), (14, 15, 30)], [(21, 50, 5), (7, 0, 45)]]\n",
      "Starting evolution...\n",
      "Evaluated 64 individuals\n",
      "--Generation: 0--\n",
      "Evaluated 54 individuals\n",
      "Min: -1.5814982706294716\n",
      "Max: 46.971697352917694\n",
      "Avg: 12.72919427780735\n",
      "Std: 15.805090882938956\n",
      "Current evaluations :  118\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -6.265625249999998\n",
      "Std drawdown: 7.361558892759412\n",
      "--Generation: 1--\n",
      "Evaluated 62 individuals\n",
      "Min: 0.5593611366527143\n",
      "Max: 46.971697352917694\n",
      "Avg: 25.998708902700574\n",
      "Std: 15.636433399285249\n",
      "Current evaluations :  180\n",
      "Min drawdown: -17.359999999999985\n",
      "Max drawdown: 0.0\n",
      "Avg drawdown: -5.958125187499998\n",
      "Std drawdown: 7.630551805821805\n",
      "--Generation: 2--\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb Cell 22\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=186'>187</a>\u001b[0m         data_dictionary[df_name] \u001b[39m=\u001b[39m data\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=188'>189</a>\u001b[0m \u001b[39mfor\u001b[39;00m df_name \u001b[39min\u001b[39;00m data_dictionary:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=189'>190</a>\u001b[0m     algorithm(data_dictionary[df_name])\n",
      "\u001b[1;32m/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb Cell 22\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m behaviours \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m ind: toolbox\u001b[39m.\u001b[39msimulate(ind, data), invalid_ind)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m fitnesses \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m bhv: toolbox\u001b[39m.\u001b[39mevaluate(bhv[\u001b[39m0\u001b[39m], bhv[\u001b[39m1\u001b[39m], data), behaviours)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(invalid_ind, fitnesses):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalues \u001b[39m=\u001b[39m fit\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m evaluations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(invalid_ind)\n",
      "\u001b[1;32m/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# evaluate the individuals with an invalid fitness\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m invalid_ind \u001b[39m=\u001b[39m [ind \u001b[39mfor\u001b[39;00m ind \u001b[39min\u001b[39;00m offspring \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ind\u001b[39m.\u001b[39mfitness\u001b[39m.\u001b[39mvalid]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m behaviours \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m ind: toolbox\u001b[39m.\u001b[39;49msimulate(ind, data), invalid_ind)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m fitnesses \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m bhv: toolbox\u001b[39m.\u001b[39mevaluate(bhv[\u001b[39m0\u001b[39m], bhv[\u001b[39m1\u001b[39m], data), behaviours)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mfor\u001b[39;00m ind, fit \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(invalid_ind, fitnesses):\n",
      "\u001b[1;32m/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# check for each long buy the next long sell\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m index, row \u001b[39min\u001b[39;00m long_buys\u001b[39m.\u001b[39miterrows():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     candidate_sells \u001b[39m=\u001b[39m data[(data[\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m>\u001b[39;49m row[\u001b[39m'\u001b[39;49m\u001b[39mDate\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m&\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                            (data[rsi_period_long] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m upper_long)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39mif\u001b[39;00m (candidate_sells\u001b[39m.\u001b[39mempty):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/haohua/Downloads/evolutionary-algorithms-for-stock-market-main/multi-objective.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         new_sell \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(data\u001b[39m.\u001b[39miloc[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/ops/common.py:81\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     79\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 81\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/arraylike.py:70\u001b[0m, in \u001b[0;36mOpsMixin.__and__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__and__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__and__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_logical_method(other, operator\u001b[39m.\u001b[39;49mand_)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:6102\u001b[0m, in \u001b[0;36mSeries._logical_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_logical_method\u001b[39m(\u001b[39mself\u001b[39m, other, op):\n\u001b[1;32m   6101\u001b[0m     res_name \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39mget_op_result_name(\u001b[39mself\u001b[39m, other)\n\u001b[0;32m-> 6102\u001b[0m     \u001b[39mself\u001b[39m, other \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49malign_method_SERIES(\u001b[39mself\u001b[39;49m, other, align_asobject\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m   6104\u001b[0m     lvalues \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values\n\u001b[1;32m   6105\u001b[0m     rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/ops/__init__.py:165\u001b[0m, in \u001b[0;36malign_method_SERIES\u001b[0;34m(left, right, align_asobject)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[39m# ToDo: Different from align_method_FRAME, list, tuple and ndarray\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m# are not coerced here\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m# because Series has inconsistencies described in #13637\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(right, ABCSeries):\n\u001b[1;32m    164\u001b[0m     \u001b[39m# avoid repeated alignment\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m left\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mequals(right\u001b[39m.\u001b[39;49mindex):\n\u001b[1;32m    166\u001b[0m         \u001b[39mif\u001b[39;00m align_asobject:\n\u001b[1;32m    167\u001b[0m             \u001b[39m# to keep original value's dtype for bool ops\u001b[39;00m\n\u001b[1;32m    168\u001b[0m             left \u001b[39m=\u001b[39m left\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/range.py:490\u001b[0m, in \u001b[0;36mRangeIndex.equals\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[39mDetermines if two Index objects contain the same elements.\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, RangeIndex):\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range \u001b[39m==\u001b[39;49m other\u001b[39m.\u001b[39;49m_range\n\u001b[1;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mequals(other)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def algorithm(data):\n",
    "    list_of_output = []\n",
    "    convergence = 5 # maximum number of generations without improvement\n",
    "    std_threshold = 1e-4 # threshold for standard deviation\n",
    "    \n",
    "    for i in range(MAX_RUNS):\n",
    "        count = 0 # counter for convergence\n",
    "        random.seed(i) # set random seed for each run\n",
    "        print('--Run: {0}--\\n'.format(i))\n",
    "        evaluations = 0\n",
    "        # create the initial population\n",
    "        pop = toolbox.population(n=POPULATION_SIZE)\n",
    "        print(pop)\n",
    "        \n",
    "        print('Starting evolution...')\n",
    "\n",
    "        # evaluate the entire population\n",
    "        behaviours = list(map(lambda ind: toolbox.simulate(ind, data), pop))\n",
    "        fitnesses= list(map(lambda bhv: toolbox.evaluate(bhv[0], bhv[1], data), behaviours)) \n",
    "        evaluations += len(pop)\n",
    "        \n",
    "        for ind, fit in zip(pop, fitnesses):\n",
    "            ind.fitness.values = fit\n",
    "        \n",
    "        pop = toolbox.select(pop, len(pop))\n",
    "\n",
    "        print('Evaluated {0} individuals'.format(len(pop)))\n",
    "        prev_min = 5000\n",
    "        for g in range(GENERATIONS):\n",
    "            print('--Generation: {0}--'.format(g))\n",
    "            # select the next generation individuals\n",
    "            # Vary the population\n",
    "            offspring = tools.selTournamentDCD(pop, len(pop))\n",
    "            # clone the selected individuals\n",
    "            offspring = list(map(toolbox.clone, offspring))\n",
    "            # apply crossover and mutation on the offspring\n",
    "            for child1, child2 in zip(offspring[::2], offspring[1::2]):\n",
    "                if random.random() < CROSSOVER_PROBABILITY:\n",
    "                    toolbox.mate(child1, child2, 'two-point')\n",
    "                    del child1.fitness.values\n",
    "                    del child2.fitness.values\n",
    "            for mutant in offspring:\n",
    "                if random.random() < MUTATION_PROBABILITY:\n",
    "                    toolbox.mutate(mutant, 'inversion')\n",
    "                    del mutant.fitness.values\n",
    "\n",
    "            # evaluate the individuals with an invalid fitness\n",
    "            invalid_ind = [ind for ind in offspring if not ind.fitness.valid]\n",
    "            behaviours = map(lambda ind: toolbox.simulate(ind, data), invalid_ind)\n",
    "            fitnesses = map(lambda bhv: toolbox.evaluate(bhv[0], bhv[1], data), behaviours)\n",
    "            for ind, fit in zip(invalid_ind, fitnesses):\n",
    "                ind.fitness.values = fit\n",
    "            evaluations += len(invalid_ind)\n",
    "            print('Evaluated {0} individuals'.format(len(invalid_ind)))\n",
    "\n",
    "            pop = toolbox.select(pop + offspring, POPULATION_SIZE)\n",
    "            # pop[:] = offspring\n",
    "\n",
    "            fits = [ind.fitness.values[0] for ind in pop]\n",
    "            fits2 = [ind.fitness.values[1] for ind in pop]\n",
    "            \n",
    "\n",
    "            length = len(pop)\n",
    "            mean = sum(fits) / length\n",
    "            sum2 = sum(x*x for x in fits)\n",
    "            std = abs(sum2 / length - mean**2)**0.5\n",
    "\n",
    "            print('Min: {0}'.format(min(fits)))\n",
    "            print('Max: {0}'.format(max(fits)))\n",
    "            print('Avg: {0}'.format(mean))\n",
    "            print('Std: {0}'.format(std))\n",
    "            print('Current evaluations : ', evaluations)\n",
    "\n",
    "            mean_drawdown = sum(fits2) / length\n",
    "            sum2_drawdown = sum(x*x for x in fits2)\n",
    "            std_drawdown = abs(sum2_drawdown / length - mean_drawdown**2)**0.5\n",
    "            print('Min drawdown: {0}'.format(min(fits2)))\n",
    "            print('Max drawdown: {0}'.format(max(fits2)))\n",
    "            print('Avg drawdown: {0}'.format(mean_drawdown))\n",
    "            print('Std drawdown: {0}'.format(std_drawdown))\n",
    "            \n",
    "            dictionary = {\n",
    "                'DATASET': df_name,\n",
    "                'RUN': i,\n",
    "                'MIN': min(fits),\n",
    "                'MAX': max(fits),\n",
    "                'AVG': mean,\n",
    "                'STD': std,\n",
    "                'MIN_DRAWDOWN': min(fits2),\n",
    "                'MAX_DRAWDOWN': max(fits2),\n",
    "                'AVG_DRAWDOWN': mean_drawdown,\n",
    "                'STD_DRAWDOWN': std_drawdown,\n",
    "                'best_individual': pop[np.argmax(fits)]\n",
    "\n",
    "            }\n",
    "            if (min(fits) == prev_min):\n",
    "                count +=1\n",
    "            prev_min = min(fits)\n",
    "\n",
    "            if (count == convergence or evaluations > EVALUATIONS):\n",
    "                list_of_output.append(dictionary)\n",
    "                print('-- End of (successful) evolution --')\n",
    "                break\n",
    "    df = pd.DataFrame(list_of_output)       \n",
    "    df.to_csv('results.csv', mode='a', header=False, index=False)    \n",
    "\n",
    "    # obtain best individuals in each run for test scheme\n",
    "    best_individuals = df['best_individual'].tolist()\n",
    "    # save to csv\n",
    "    df = pd.DataFrame(best_individuals)\n",
    "    df.to_csv('best_individuals_multi.csv', mode='a', header=False, index=False)\n",
    "\n",
    "    # obtain averages of the runs and minimums, maximums, averages, and standard deviations of the best individuals\n",
    "    min_values = [item['MIN'] for item in list_of_output]\n",
    "    max_values = [item['MAX'] for item in list_of_output]\n",
    "    avg_values = [item['AVG'] for item in list_of_output]\n",
    "    std_values = [item['STD'] for item in list_of_output]\n",
    "\n",
    "    min_average = sum(min_values) / len(min_values)\n",
    "    max_average = sum(max_values) / len(max_values)\n",
    "    avg_average = sum(avg_values) / len(avg_values)\n",
    "    std_average = sum(std_values) / len(std_values)\n",
    "\n",
    "    min_from_output = min(list_of_output, key=lambda x:x['MIN'])\n",
    "    max_from_output = max(list_of_output, key=lambda x:x['MAX'])\n",
    "    avg_from_output = max(list_of_output, key=lambda x:x['AVG'])\n",
    "    std_from_output = min(list_of_output, key=lambda x:x['STD'])\n",
    "\n",
    "\n",
    "    min_values_drawdown = [item['MIN_DRAWDOWN'] for item in list_of_output]\n",
    "    max_values_drawdown = [item['MAX_DRAWDOWN'] for item in list_of_output]\n",
    "    avg_values_drawdown = [item['AVG_DRAWDOWN'] for item in list_of_output]\n",
    "    std_values_drawdown = [item['STD_DRAWDOWN'] for item in list_of_output]\n",
    "    min_average_drawdown = sum(min_values_drawdown) / len(min_values_drawdown)\n",
    "    max_average_drawdown = sum(max_values_drawdown) / len(max_values_drawdown)\n",
    "    avg_average_drawdown = sum(avg_values_drawdown) / len(avg_values_drawdown)\n",
    "    std_average_drawdown = sum(std_values_drawdown) / len(std_values_drawdown)\n",
    "\n",
    "    min_from_output_drawdown = min(list_of_output, key=lambda x:x['MIN_DRAWDOWN'])\n",
    "    max_from_output_drawdown = max(list_of_output, key=lambda x:x['MAX_DRAWDOWN'])\n",
    "    avg_from_output_drawdown = max(list_of_output, key=lambda x:x['AVG_DRAWDOWN'])\n",
    "    std_from_output_drawdown = min(list_of_output, key=lambda x:x['STD_DRAWDOWN'])\n",
    "    \n",
    "\n",
    "    dictionary_output = {\n",
    "        'DATASET': df_name,\n",
    "        'MIN': min_from_output['MIN'],\n",
    "        'MAX': max_from_output['MAX'],\n",
    "        'AVG': avg_from_output['AVG'],\n",
    "        'STD': std_from_output['STD'],\n",
    "        'AVG_MIN': min_average,\n",
    "        'AVG_MAX': max_average,\n",
    "        'AVG_AVG': avg_average,\n",
    "        'AVG_STD': std_average,\n",
    "        'MIN_DRAWDOWN': min_from_output_drawdown['MIN_DRAWDOWN'],\n",
    "        'MAX_DRAWDOWN': max_from_output_drawdown['MAX_DRAWDOWN'],\n",
    "        'AVG_DRAWDOWN': avg_from_output_drawdown['AVG_DRAWDOWN'],\n",
    "        'STD_DRAWDOWN': std_from_output_drawdown['STD_DRAWDOWN'],\n",
    "        'AVG_MIN_DRAWDOWN': min_average_drawdown,\n",
    "        'AVG_MAX_DRAWDOWN': max_average_drawdown,\n",
    "        'AVG_AVG_DRAWDOWN': avg_average_drawdown,\n",
    "        'AVG_MIN_DRAWDOWN': min_average_drawdown,\n",
    "    }\n",
    "    # save results for table in report\n",
    "    df2 = pd.DataFrame(dictionary_output, index=[0])\n",
    "    df2.to_csv('results_for_table_multi.csv', mode='a', header=False, index=False)\n",
    "\n",
    "            \n",
    "folder_path = 'data'\n",
    "\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "data_dictionary = {}\n",
    "RSI_dictionary = {}\n",
    "price_dictionary = {}\n",
    "date_dictionary = {}\n",
    "\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.csv'):\n",
    "        # extract the base name of the file (without the extension)\n",
    "        df_name = os.path.splitext(file_name)[0]\n",
    "        # construct the full file path\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        # read the CSV file into a DataFrame with the base name as the variable name\n",
    "        data_dictionary[df_name] = pd.read_csv(file_path, sep=';', usecols=['Date', 'Close'])\n",
    "        data = process_data(data_dictionary[df_name])\n",
    "        data_dictionary[df_name] = data\n",
    "\n",
    "for df_name in data_dictionary:\n",
    "    algorithm(data_dictionary[df_name])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
